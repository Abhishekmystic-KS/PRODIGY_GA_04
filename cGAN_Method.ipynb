{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea31f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Select the computational device (GPU T4 x2 is ideal on Kaggle)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION & HYPERPARAMETERS ---\n",
    "\n",
    "# Image settings\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "CHANNELS_IMG = 3\n",
    "\n",
    "# Training settings\n",
    "BATCH_SIZE = 16          # Number of images processed at once\n",
    "LEARNING_RATE = 2e-4     # How fast the optimizer updates weights\n",
    "BETA1 = 0.5              # Specific setting for the Adam optimizer\n",
    "BETA2 = 0.999            # Specific setting for the Adam optimizer\n",
    "NUM_EPOCHS = 100         # Total passes through the entire dataset\n",
    "L1_LAMBDA = 100          # Weight for L1 loss (reconstruction quality)\n",
    "\n",
    "# Path to your dataset on Kaggle (Update this based on your selected data)\n",
    "# Example for CMP Facades: '/kaggle/input/facades-dataset/facades/train/'\n",
    "DATA_PATH = \"/kaggle/input/notebooks/japanajitsinghgandhi/pix2pix-facades-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacadesDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        # Only include actual image files\n",
    "        self.list_files = [f for f in os.listdir(self.root_dir) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_file = self.list_files[index]\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "        \n",
    "        # 1. Open and ensure RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # 2. Split the side-by-side image (Target is left, Input is right)\n",
    "        # These MUST be defined before the transform\n",
    "        target_img_arr = image[:, :256, :]\n",
    "        input_img_arr = image[:, 256:, :]\n",
    "\n",
    "        # 3. Define the transform with Resize to prevent U-Net dimension errors\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        # 4. Apply transforms and return\n",
    "        return transform(input_img_arr), transform(target_img_arr)\n",
    "\n",
    "# Re-initialize the loader\n",
    "train_dataset = FacadesDataset(root_dir='/kaggle/input/notebooks/japanajitsinghgandhi/pix2pix-facades-dataset')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"âœ… Ready! Loaded {len(train_dataset)} image pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for Downsampling (Encoder)\n",
    "def down_block(in_c, out_c, normalize=True, dropout=0.0):\n",
    "    layers = [nn.Conv2d(in_c, out_c, 4, stride=2, padding=1, bias=False)]\n",
    "    if normalize:\n",
    "        layers.append(nn.BatchNorm2d(out_c))\n",
    "    layers.append(nn.LeakyReLU(0.2))\n",
    "    if dropout:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Helper for Upsampling (Decoder)\n",
    "def up_block(in_c, out_c, dropout=0.0):\n",
    "    layers = [\n",
    "        nn.ConvTranspose2d(in_c, out_c, 4, stride=2, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True)\n",
    "    ]\n",
    "    if dropout:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Encoder (Contracting Path)\n",
    "        self.d1 = down_block(3, 64, normalize=False) # 128x128\n",
    "        self.d2 = down_block(64, 128)               # 64x64\n",
    "        self.d3 = down_block(128, 256)              # 32x32\n",
    "        self.d4 = down_block(256, 512, dropout=0.5) # 16x16\n",
    "        self.d5 = down_block(512, 512, dropout=0.5) # 8x8\n",
    "        self.d6 = down_block(512, 512, dropout=0.5) # 4x4\n",
    "        self.d7 = down_block(512, 512, dropout=0.5) # 2x2\n",
    "        self.d8 = down_block(512, 512, normalize=False) # 1x1\n",
    "\n",
    "        # Decoder (Expanding Path) with Skip Connections\n",
    "        self.u1 = up_block(512, 512, dropout=0.5)      # 2x2\n",
    "        self.u2 = up_block(1024, 512, dropout=0.5)     # 4x4\n",
    "        self.u3 = up_block(1024, 512, dropout=0.5)     # 8x8\n",
    "        self.u4 = up_block(1024, 512, dropout=0.5)     # 16x16\n",
    "        self.u5 = up_block(1024, 256)                  # 32x32\n",
    "        self.u6 = up_block(512, 128)                   # 64x64\n",
    "        self.u7 = up_block(256, 64)                    # 128x128\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh() # Output range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        en1 = self.d1(x)\n",
    "        en2 = self.d2(en1)\n",
    "        en3 = self.d3(en2)\n",
    "        en4 = self.d4(en3)\n",
    "        en5 = self.d5(en4)\n",
    "        en6 = self.d6(en5)\n",
    "        en7 = self.d7(en6)\n",
    "        en8 = self.d8(en7)\n",
    "\n",
    "        # Decoder with Skip Connections (Concatenation)\n",
    "        de1 = self.u1(en8)\n",
    "        de2 = self.u2(torch.cat([de1, en7], 1))\n",
    "        de3 = self.u3(torch.cat([de2, en6], 1))\n",
    "        de4 = self.u4(torch.cat([de3, en5], 1))\n",
    "        de5 = self.u5(torch.cat([de4, en4], 1))\n",
    "        de6 = self.u6(torch.cat([de5, en3], 1))\n",
    "        de7 = self.u7(torch.cat([de6, en2], 1))\n",
    "        \n",
    "        return self.final(torch.cat([de7, en1], 1))\n",
    "\n",
    "# Initialize\n",
    "gen = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7516191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOSS FUNCTIONS ---\n",
    "# Binary Cross Entropy with Logits for the Adversarial Loss\n",
    "# This measures how well the Generator fools the Discriminator\n",
    "BCE_LOSS = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# L1 Loss for the Reconstruction Loss\n",
    "# This measures how physically similar the generated image is to the real target\n",
    "L1_LOSS = nn.L1Loss()\n",
    "\n",
    "# --- OPTIMIZERS ---\n",
    "# We use the Adam optimizer, which is the standard for GANs\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "disc_optimizer = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "\n",
    "# --- UPDATED: SCALER FOR MIXED PRECISION ---\n",
    "# Using the new PyTorch 2.x syntax\n",
    "scaler_gen = torch.amp.GradScaler('cuda')\n",
    "scaler_disc = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aded3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # PatchGAN takes both the Input Image and the Target Image concatenated\n",
    "        # So in_channels = 3 (input) + 3 (target) = 6\n",
    "        self.model = nn.Sequential(\n",
    "            down_block(6, 64, normalize=False), # 128x128\n",
    "            down_block(64, 128),               # 64x64\n",
    "            down_block(128, 256),              # 32x32\n",
    "            down_block(256, 512),              # 31x31 (stride=1 in final layers)\n",
    "            nn.Conv2d(512, 1, 4, padding=1)    # 30x30 output patch\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x is the input sketch, y is the generated/real image\n",
    "        # We concatenate them along the channel dimension\n",
    "        input_data = torch.cat([x, y], dim=1)\n",
    "        return self.model(input_data)\n",
    "\n",
    "# Initialize\n",
    "disc = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(gen, input_image, target_image, epoch):\n",
    "    gen.eval() # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Generate the fake image\n",
    "        fake_image = gen(input_image)\n",
    "        \n",
    "        # Denormalize images from [-1, 1] back to [0, 1] for plotting\n",
    "        input_plot = input_image[0] * 0.5 + 0.5\n",
    "        target_plot = target_image[0] * 0.5 + 0.5\n",
    "        fake_plot = fake_image[0] * 0.5 + 0.5\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        titles = ['Input Sketch', 'Target Photo', 'Generated Photo']\n",
    "        images = [input_plot, target_plot, fake_plot]\n",
    "        \n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.title(f\"{titles[i]} (Epoch {epoch})\")\n",
    "            plt.imshow(images[i].permute(1, 2, 0).cpu().numpy())\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "    gen.train() # Set back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d96bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    for idx, (input_img, target_img) in enumerate(loop):\n",
    "        input_img, target_img = input_img.to(device), target_img.to(device)\n",
    "\n",
    "        # --- TRAIN DISCRIMINATOR ---\n",
    "        disc_optimizer.zero_grad()\n",
    "        # We still use autocast for speed, but perform standard backward/step\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            fake_img = gen(input_img)\n",
    "            disc_real = disc(input_img, target_img)\n",
    "            disc_fake = disc(input_img, fake_img.detach())\n",
    "            \n",
    "            disc_real_loss = BCE_LOSS(disc_real, torch.ones_like(disc_real))\n",
    "            disc_fake_loss = BCE_LOSS(disc_fake, torch.zeros_like(disc_fake))\n",
    "            disc_loss = (disc_real_loss + disc_fake_loss) / 2\n",
    "\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # --- TRAIN GENERATOR ---\n",
    "        gen_optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            disc_fake = disc(input_img, fake_img)\n",
    "            gen_fake_loss = BCE_LOSS(disc_fake, torch.ones_like(disc_fake))\n",
    "            l1_loss = L1_LOSS(fake_img, target_img) * L1_LAMBDA\n",
    "            gen_loss = gen_fake_loss + l1_loss\n",
    "\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "        if idx % 10 == 0:\n",
    "            loop.set_postfix(D_loss=disc_loss.item(), G_loss=gen_loss.item())\n",
    "\n",
    "    # Show results at the end of each epoch\n",
    "    show_progress(gen, input_img, target_img, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
